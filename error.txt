(sa2) G:\My Drive\01_files\001_Studium\001 Semester\05 HS24\08_SA2\TermPaper2_RasterClassification\code>python run_model.py --device=gpu --num_workers=4 --patience=20 --batch_size=16 --sample_data --overwrite
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
`Trainer(limit_test_batches=1.0)` was configured so 100% of the batches will be used..
`Trainer(limit_predict_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type             | Params | Mode
-------------------------------------------------------
0 | model     | ResNet           | 11.4 K | train
1 | criterion | CrossEntropyLoss | 0      | train
-------------------------------------------------------
11.4 K    Trainable params
0         Non-trainable params
11.4 K    Total params
0.046     Total estimated model params size (MB)
53        Modules in train mode
0         Modules in eval mode
Sanity Checking: |                                                                                                                                                                                            | 0/? [00:00<?, ?it/s]C:\Users\kraft\.conda\envs\sa2\Lib\site-packages\lightning\pytorch\trainer\connectors\data_connector.py:420: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.
C:\Users\kraft\.conda\envs\sa2\Lib\site-packages\lightning\pytorch\trainer\connectors\data_connector.py:420: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.      
Epoch 0: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 158/158 [00:25<00:00,  6.12it/s] 
Epoch 12:  12%|█████████████████████▋                                                                                                                                                              | 19/158 [00:28<03:30,  0.66it/s]C:\cb\pytorch_1000000000000\work\aten\src\ATen\native\cuda\Loss.cu:250: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.                                                                                   
Traceback (most recent call last):
  File "C:\Users\kraft\.conda\envs\sa2\Lib\site-packages\lightning\pytorch\trainer\call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kraft\.conda\envs\sa2\Lib\site-packages\lightning\pytorch\trainer\trainer.py", line 575, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "C:\Users\kraft\.conda\envs\sa2\Lib\site-packages\lightning\pytorch\trainer\trainer.py", line 982, in _run
    results = self._run_stage()
              ^^^^^^^^^^^^^^^^^
  File "C:\Users\kraft\.conda\envs\sa2\Lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1026, in _run_stage
    self.fit_loop.run()
  File "C:\Users\kraft\.conda\envs\sa2\Lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 216, in run
    self.advance()
  File "C:\Users\kraft\.conda\envs\sa2\Lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 455, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "C:\Users\kraft\.conda\envs\sa2\Lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 150, in run
    self.advance(data_fetcher)
  File "C:\Users\kraft\.conda\envs\sa2\Lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 320, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kraft\.conda\envs\sa2\Lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 192, in run
    self._optimizer_step(batch_idx, closure)
  File "C:\Users\kraft\.conda\envs\sa2\Lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 270, in _optimizer_step
    call._call_lightning_module_hook(
  File "C:\Users\kraft\.conda\envs\sa2\Lib\site-packages\lightning\pytorch\trainer\call.py", line 171, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kraft\.conda\envs\sa2\Lib\site-packages\lightning\pytorch\core\module.py", line 1302, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "C:\Users\kraft\.conda\envs\sa2\Lib\site-packages\lightning\pytorch\core\optimizer.py", line 154, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kraft\.conda\envs\sa2\Lib\site-packages\lightning\pytorch\strategies\strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kraft\.conda\envs\sa2\Lib\site-packages\lightning\pytorch\plugins\precision\precision.py", line 123, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kraft\.conda\envs\sa2\Lib\site-packages\torch\optim\optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kraft\.conda\envs\sa2\Lib\site-packages\torch\optim\optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kraft\.conda\envs\sa2\Lib\site-packages\torch\optim\adamw.py", line 197, in step
    loss = closure()
           ^^^^^^^^^
  File "C:\Users\kraft\.conda\envs\sa2\Lib\site-packages\lightning\pytorch\plugins\precision\precision.py", line 109, in _wrap_closure
    closure_result = closure()
                     ^^^^^^^^^
  File "C:\Users\kraft\.conda\envs\sa2\Lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 146, in __call__
    self._result = self.closure(*args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kraft\.conda\envs\sa2\Lib\site-packages\torch\utils\_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kraft\.conda\envs\sa2\Lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 140, in closure
    self._backward_fn(step_output.closure_loss)
  File "C:\Users\kraft\.conda\envs\sa2\Lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 241, in backward_fn
    call._call_strategy_hook(self.trainer, "backward", loss, optimizer)
  File "C:\Users\kraft\.conda\envs\sa2\Lib\site-packages\lightning\pytorch\trainer\call.py", line 323, in _call_strategy_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kraft\.conda\envs\sa2\Lib\site-packages\lightning\pytorch\strategies\strategy.py", line 213, in backward
    self.precision_plugin.backward(closure_loss, self.lightning_module, optimizer, *args, **kwargs)
  File "C:\Users\kraft\.conda\envs\sa2\Lib\site-packages\lightning\pytorch\plugins\precision\precision.py", line 73, in backward
    model.backward(tensor, *args, **kwargs)
  File "C:\Users\kraft\.conda\envs\sa2\Lib\site-packages\lightning\pytorch\core\module.py", line 1097, in backward
    loss.backward(*args, **kwargs)
  File "C:\Users\kraft\.conda\envs\sa2\Lib\site-packages\torch\_tensor.py", line 581, in backward
    torch.autograd.backward(
  File "C:\Users\kraft\.conda\envs\sa2\Lib\site-packages\torch\autograd\__init__.py", line 347, in backward
    _engine_run_backward(
  File "C:\Users\kraft\.conda\envs\sa2\Lib\site-packages\torch\autograd\graph.py", line 825, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: cuDNN error: CUDNN_STATUS_BAD_PARAM_STREAM_MISMATCH

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "G:\My Drive\01_files\001_Studium\001 Semester\05 HS24\08_SA2\TermPaper2_RasterClassification\code\run_model.py", line 153, in <module>
    trainer.fit(model=model, datamodule=datamodule)
  File "C:\Users\kraft\.conda\envs\sa2\Lib\site-packages\lightning\pytorch\trainer\trainer.py", line 539, in fit
    call._call_and_handle_interrupt(
  File "C:\Users\kraft\.conda\envs\sa2\Lib\site-packages\lightning\pytorch\trainer\call.py", line 68, in _call_and_handle_interrupt
    trainer._teardown()
  File "C:\Users\kraft\.conda\envs\sa2\Lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1005, in _teardown
    self.strategy.teardown()
  File "C:\Users\kraft\.conda\envs\sa2\Lib\site-packages\lightning\pytorch\strategies\strategy.py", line 532, in teardown
    _optimizers_to_device(self.optimizers, torch.device("cpu"))
  File "C:\Users\kraft\.conda\envs\sa2\Lib\site-packages\lightning\fabric\utilities\optimizer.py", line 27, in _optimizers_to_device
    _optimizer_to_device(opt, device)
  File "C:\Users\kraft\.conda\envs\sa2\Lib\site-packages\lightning\fabric\utilities\optimizer.py", line 41, in _optimizer_to_device
    v[key] = move_data_to_device(val, device)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kraft\.conda\envs\sa2\Lib\site-packages\lightning\fabric\utilities\apply_func.py", line 110, in move_data_to_device
    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kraft\.conda\envs\sa2\Lib\site-packages\lightning_utilities\core\apply_func.py", line 64, in apply_to_collection
    return function(data, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kraft\.conda\envs\sa2\Lib\site-packages\lightning\fabric\utilities\apply_func.py", line 104, in batch_to
    data_output = data.to(device, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Epoch 12:  12%|█▏        | 19/158 [00:29<03:36,  0.64it/s]
